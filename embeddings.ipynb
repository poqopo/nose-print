{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "281def09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49de5924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수: 20,000\n",
      "Train: 13,956, Validation: 6,044\n",
      "Validation 비율: 0.302, Validation에 포함된 클래스 수: 6,000\n",
      "Triplet 학습에 사용할 샘플: 12,185 (제외 1,771)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dog_id</th>\n",
       "      <th>image_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4651</td>\n",
       "      <td>A*H6e0QKriV2QAAAAAAAAAAAAAAQAAAQ.jpg</td>\n",
       "      <td>4651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5144</td>\n",
       "      <td>A*B1jXQrueubcAAAAAAAAAAAAAAQAAAQ.jpg</td>\n",
       "      <td>5144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2148</td>\n",
       "      <td>A*xRIKTrCpOy4_9YRBiXiZ3QAAAQAAAQ.jpg</td>\n",
       "      <td>2148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5705</td>\n",
       "      <td>A*GGwZR7_S7yYAAAAAAAAAAAAAAQAAAQ.jpg</td>\n",
       "      <td>5705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3628</td>\n",
       "      <td>A*8JIqQpZyc3AAAAAAAAAAAAAAAQAAAQ.jpg</td>\n",
       "      <td>3628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dog_id                            image_name  label\n",
       "0    4651  A*H6e0QKriV2QAAAAAAAAAAAAAAQAAAQ.jpg   4651\n",
       "1    5144  A*B1jXQrueubcAAAAAAAAAAAAAAQAAAQ.jpg   5144\n",
       "2    2148  A*xRIKTrCpOy4_9YRBiXiZ3QAAAQAAAQ.jpg   2148\n",
       "3    5705  A*GGwZR7_S7yYAAAAAAAAAAAAAAQAAAQ.jpg   5705\n",
       "4    3628  A*8JIqQpZyc3AAAAAAAAAAAAAAAQAAAQ.jpg   3628"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"./\")\n",
    "IMAGE_DIR = DATA_ROOT / \"images\"\n",
    "META_PATH = DATA_ROOT / \"data.csv\"\n",
    "RANDOM_SEED = 42\n",
    "VAL_RATIO = 0.2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_SIZE = 224\n",
    "EMBED_DIM = 256\n",
    "TRIPLET_MARGIN = 0.3\n",
    "LOG_INTERVAL = 50\n",
    "TOP_K = 1\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "assert META_PATH.exists(), f\"Metadata not found: {META_PATH}\"\n",
    "assert IMAGE_DIR.exists(), f\"Image directory not found: {IMAGE_DIR}\"\n",
    "\n",
    "df = pd.read_csv(META_PATH)\n",
    "df = df.rename(columns={\"dog ID\": \"dog_id\", \"nose print image\": \"image_name\"})\n",
    "\n",
    "available_mask = df[\"image_name\"].apply(lambda name: (IMAGE_DIR / name).exists())\n",
    "missing = df.loc[~available_mask, \"image_name\"].tolist()\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"{len(missing)} images missing. Example: {missing[:3]}\")\n",
    "\n",
    "label_map = {dog_id: idx for idx, dog_id in enumerate(sorted(df[\"dog_id\"].unique()))}\n",
    "id_map = {idx: dog_id for dog_id, idx in label_map.items()}\n",
    "df[\"label\"] = df[\"dog_id\"].map(label_map)\n",
    "\n",
    "\n",
    "def stratified_per_class_split(frame: pd.DataFrame, val_ratio: float, seed: int):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    train_parts = []\n",
    "    val_parts = []\n",
    "\n",
    "    for dog_id, group in frame.groupby(\"dog_id\"):\n",
    "        indices = group.index.to_numpy()\n",
    "        rng.shuffle(indices)\n",
    "\n",
    "        if len(group) == 1:\n",
    "            train_parts.append(group)\n",
    "            continue\n",
    "\n",
    "        val_count = max(1, int(round(len(group) * val_ratio)))\n",
    "        val_count = min(len(group) - 1, val_count)\n",
    "\n",
    "        val_idx = indices[:val_count]\n",
    "        train_idx = indices[val_count:]\n",
    "\n",
    "        train_parts.append(frame.loc[train_idx])\n",
    "        val_parts.append(frame.loc[val_idx])\n",
    "\n",
    "    train_df = pd.concat(train_parts).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    if val_parts:\n",
    "        val_df = pd.concat(val_parts).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    else:\n",
    "        val_df = pd.DataFrame(columns=frame.columns)\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "train_df, val_df = stratified_per_class_split(df, VAL_RATIO, RANDOM_SEED)\n",
    "\n",
    "triplet_mask = train_df.groupby(\"dog_id\")[\"image_name\"].transform(\"count\") >= 2\n",
    "triplet_train_df = train_df.loc[triplet_mask].reset_index(drop=True)\n",
    "excluded = len(train_df) - len(triplet_train_df)\n",
    "\n",
    "print(f\"총 샘플 수: {len(df):,}\")\n",
    "print(f\"Train: {len(train_df):,}, Validation: {len(val_df):,}\")\n",
    "print(\n",
    "    f\"Validation 비율: {len(val_df) / len(df):.3f}, \"\n",
    "    f\"Validation에 포함된 클래스 수: {val_df['dog_id'].nunique():,}\"\n",
    ")\n",
    "print(f\"Triplet 학습에 사용할 샘플: {len(triplet_train_df):,} (제외 {excluded:,})\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59edc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12185, 13956, 6044)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DogNoseDataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame, image_dir: Path, transform=None):\n",
    "        self.frame = frame.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, int]:\n",
    "        row = self.frame.iloc[idx]\n",
    "        image_path = self.image_dir / row[\"image_name\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = int(row[\"label\"])\n",
    "        return image, label, idx\n",
    "\n",
    "\n",
    "class TripletDogDataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame, image_dir: Path, transform=None, seed: int = RANDOM_SEED):\n",
    "        self.frame = frame.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.seed = seed\n",
    "        grouped = self.frame.groupby(\"dog_id\").indices\n",
    "        self.grouped_indices = {dog_id: np.array(indices) for dog_id, indices in grouped.items()}\n",
    "        self.dog_ids = list(self.grouped_indices.keys())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.frame)\n",
    "\n",
    "    def _sample_positive(self, dog_id: int, anchor_idx: int, rng: np.random.Generator) -> int:\n",
    "        candidates = self.grouped_indices[dog_id]\n",
    "        if len(candidates) == 1:\n",
    "            return anchor_idx\n",
    "        pos_idx = anchor_idx\n",
    "        while pos_idx == anchor_idx:\n",
    "            pos_idx = int(rng.choice(candidates))\n",
    "        return pos_idx\n",
    "\n",
    "    def _sample_negative(self, dog_id: int, rng: np.random.Generator) -> int:\n",
    "        neg_dog = dog_id\n",
    "        while neg_dog == dog_id:\n",
    "            neg_dog = rng.choice(self.dog_ids)\n",
    "        neg_idx = int(rng.choice(self.grouped_indices[neg_dog]))\n",
    "        return neg_idx\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        rng = np.random.default_rng(self.seed + idx)\n",
    "        row = self.frame.iloc[idx]\n",
    "        pos_idx = self._sample_positive(row[\"dog_id\"], idx, rng)\n",
    "        neg_idx = self._sample_negative(row[\"dog_id\"], rng)\n",
    "\n",
    "        pos_row = self.frame.iloc[pos_idx]\n",
    "        neg_row = self.frame.iloc[neg_idx]\n",
    "\n",
    "        def load_image(image_name: str):\n",
    "            image = Image.open(self.image_dir / image_name).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                return self.transform(image)\n",
    "            return transforms.ToTensor()(image)\n",
    "\n",
    "        anchor_img = load_image(row[\"image_name\"])\n",
    "        pos_img = load_image(pos_row[\"image_name\"])\n",
    "        neg_img = load_image(neg_row[\"image_name\"])\n",
    "        return anchor_img, pos_img, neg_img\n",
    "\n",
    "\n",
    "def build_transforms(image_size: int, train: bool = True):\n",
    "    base_transforms = [\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    "    if train:\n",
    "        aug = [\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        ]\n",
    "        return transforms.Compose(aug + base_transforms[1:])\n",
    "    return transforms.Compose(base_transforms)\n",
    "\n",
    "\n",
    "train_transform = build_transforms(IMAGE_SIZE, train=True)\n",
    "eval_transform = build_transforms(IMAGE_SIZE, train=False)\n",
    "\n",
    "triplet_dataset = TripletDogDataset(triplet_train_df, IMAGE_DIR, transform=train_transform, seed=RANDOM_SEED)\n",
    "train_gallery_dataset = DogNoseDataset(train_df, IMAGE_DIR, transform=eval_transform)\n",
    "val_dataset = DogNoseDataset(val_df, IMAGE_DIR, transform=eval_transform)\n",
    "\n",
    "num_workers = max(1, min(4, (os.cpu_count() or 1)))\n",
    "triplet_loader = DataLoader(\n",
    "    triplet_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "train_gallery_loader = DataLoader(\n",
    "    train_gallery_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "len(triplet_dataset), len(train_gallery_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135c8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "\n",
    "class NoseEmbeddingModel(nn.Module):\n",
    "    def __init__(self, embedding_dim: int = EMBED_DIM):\n",
    "        super().__init__()\n",
    "        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        in_features = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        emb = self.head(feats)\n",
    "        return F.normalize(emb, p=2, dim=1)\n",
    "\n",
    "\n",
    "model = NoseEmbeddingModel().to(device)\n",
    "criterion = nn.TripletMarginLoss(margin=TRIPLET_MARGIN, p=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "\n",
    "def run_metric_epoch(loader: DataLoader, epoch: int, phase: str, train: bool = True):\n",
    "    epoch_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    progress = tqdm(loader, leave=False)\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for batch_idx, (anchor, positive, negative) in enumerate(progress, start=1):\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            anchor_emb = model(anchor)\n",
    "            positive_emb = model(positive)\n",
    "            negative_emb = model(negative)\n",
    "            loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            batch_size = anchor.size(0)\n",
    "            epoch_loss += loss.item() * batch_size\n",
    "            total += batch_size\n",
    "\n",
    "            progress.set_description(f\"{phase} loss={loss.item():.4f}\")\n",
    "            if LOG_INTERVAL and (batch_idx % LOG_INTERVAL == 0 or batch_idx == len(loader)):\n",
    "                avg_loss = epoch_loss / max(total, 1)\n",
    "                tqdm.write(\n",
    "                    f\"[{phase}] Epoch {epoch} Step {batch_idx}/{len(loader)} | \"\n",
    "                    f\"Batch loss {loss.item():.4f} | Running loss {avg_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "    return epoch_loss / max(total, 1)\n",
    "\n",
    "\n",
    "def compute_embeddings(loader: DataLoader, desc: str):\n",
    "    model.eval()\n",
    "    embeddings, labels, indices = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, label_batch, index_batch in tqdm(loader, desc=desc, leave=False):\n",
    "            images = images.to(device)\n",
    "            emb = model(images).cpu()\n",
    "            embeddings.append(emb)\n",
    "            labels.append(label_batch.cpu())\n",
    "            indices.append(index_batch.cpu())\n",
    "    if embeddings:\n",
    "        return torch.cat(embeddings), torch.cat(labels), torch.cat(indices)\n",
    "    return (\n",
    "        torch.empty(0, EMBED_DIM),\n",
    "        torch.empty(0, dtype=torch.long),\n",
    "        torch.empty(0, dtype=torch.long),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_retrieval(epoch: int):\n",
    "    train_embs, train_labels, train_indices = compute_embeddings(train_gallery_loader, \"Train embed\")\n",
    "    val_embs, val_labels, val_indices = compute_embeddings(val_loader, \"Val embed\")\n",
    "    if len(train_embs) == 0 or len(val_embs) == 0:\n",
    "        return 0.0, {}\n",
    "\n",
    "    train_norm = F.normalize(train_embs, p=2, dim=1)\n",
    "    val_norm = F.normalize(val_embs, p=2, dim=1)\n",
    "\n",
    "    similarity = val_norm @ train_norm.T\n",
    "    best_sim, best_indices = similarity.max(dim=1)\n",
    "    pred_labels = train_labels[best_indices]\n",
    "    acc = (pred_labels == val_labels).float().mean().item()\n",
    "\n",
    "    return acc, {\n",
    "        \"train_embeddings\": train_norm,\n",
    "        \"train_labels\": train_labels,\n",
    "        \"train_indices\": train_indices,\n",
    "        \"val_embeddings\": val_norm,\n",
    "        \"val_labels\": val_labels,\n",
    "        \"val_indices\": val_indices,\n",
    "        \"pred_indices\": best_indices,\n",
    "        \"pred_labels\": pred_labels,\n",
    "        \"similarity\": best_sim,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f76f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff9c3e47cb14fb996f9022c6854c686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTriplet 학습에 사용할 데이터가 없습니다. 각 dog ID에 최소 2장의 이미지가 필요합니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_metric_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriplet_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     val_acc, _ \u001b[38;5;241m=\u001b[39m evaluate_retrieval(epoch)\n\u001b[1;32m     11\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[10], line 59\u001b[0m, in \u001b[0;36mrun_metric_epoch\u001b[0;34m(loader, epoch, phase, train)\u001b[0m\n\u001b[1;32m     56\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m anchor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m     60\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m     62\u001b[0m progress\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = []\n",
    "best_state = None\n",
    "best_val_acc = 0.0\n",
    "\n",
    "if len(triplet_dataset) == 0:\n",
    "    raise ValueError(\"Triplet 학습에 사용할 데이터가 없습니다. 각 dog ID에 최소 2장의 이미지가 필요합니다.\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = run_metric_epoch(triplet_loader, epoch, \"Train\", train=True)\n",
    "    val_acc, _ = evaluate_retrieval(epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    history.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = {\n",
    "            \"model\": deepcopy(model.state_dict()),\n",
    "            \"optimizer\": deepcopy(optimizer.state_dict()),\n",
    "        }\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "        f\"Train loss {train_loss:.4f} | \"\n",
    "        f\"Val NN acc {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770067b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history)\n",
    "if not history_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    history_df.plot(x=\"epoch\", y=\"train_loss\", ax=axes[0], title=\"Triplet Loss\")\n",
    "    history_df.plot(x=\"epoch\", y=\"val_acc\", ax=axes[1], title=\"Val NN Accuracy\")\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "history_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state[\"model\"])\n",
    "    optimizer.load_state_dict(best_state[\"optimizer\"])\n",
    "    print(f\"최고 Validation NN 정확도: {best_val_acc:.4f}\")\n",
    "else:\n",
    "    print(\"경고: 최고 성능 가중치가 저장되지 않았습니다. 현재 모델 상태를 사용합니다.\")\n",
    "\n",
    "final_acc, eval_cache = evaluate_retrieval(NUM_EPOCHS + 1)\n",
    "print(f\"최종 최근접 이웃 정확도: {final_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8bfe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not eval_cache:\n",
    "    raise RuntimeError(\"평가 캐시가 비어 있습니다. 먼저 evaluate_retrieval을 실행하세요.\")\n",
    "\n",
    "train_indices = eval_cache[\"train_indices\"].numpy()\n",
    "val_indices = eval_cache[\"val_indices\"].numpy()\n",
    "pred_indices = eval_cache[\"pred_indices\"].numpy()\n",
    "pred_labels = eval_cache[\"pred_labels\"].numpy()\n",
    "\n",
    "val_results = val_dataset.frame.iloc[val_indices].reset_index(drop=True)\n",
    "val_results[\"pred_label\"] = pred_labels\n",
    "val_results[\"pred_dog_id\"] = val_results[\"pred_label\"].map(id_map)\n",
    "val_results[\"correct\"] = val_results[\"pred_label\"] == val_results[\"label\"]\n",
    "\n",
    "match_gallery = train_gallery_dataset.frame.iloc[pred_indices].reset_index(drop=True)\n",
    "val_results[\"match_image_name\"] = match_gallery[\"image_name\"].values\n",
    "val_results[\"match_dog_id\"] = match_gallery[\"dog_id\"].values\n",
    "\n",
    "val_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(df: pd.DataFrame, num_samples: int = 6):\n",
    "    if df.empty:\n",
    "        raise ValueError(\"시각화할 데이터가 없습니다.\")\n",
    "    picks = df.sample(min(num_samples, len(df)), random_state=RANDOM_SEED)\n",
    "    cols = 3\n",
    "    rows = math.ceil(len(picks) / cols)\n",
    "    plt.figure(figsize=(cols * 4.5, rows * 4.5))\n",
    "\n",
    "    for idx, (_, row) in enumerate(picks.iterrows()):\n",
    "        ax = plt.subplot(rows, cols, idx + 1)\n",
    "        query_image = Image.open(IMAGE_DIR / row[\"image_name\"]).convert(\"RGB\")\n",
    "        ax.imshow(query_image)\n",
    "        ax.axis(\"off\")\n",
    "        title = (\n",
    "            f\"정답:{row['dog_id']}\\n예측:{row['pred_dog_id']}\"\n",
    "            + (\" ✅\" if row[\"correct\"] else \" ❌\")\n",
    "        )\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return picks[[\"image_name\", \"dog_id\", \"pred_dog_id\", \"correct\", \"match_image_name\", \"match_dog_id\"]]\n",
    "\n",
    "\n",
    "show_predictions(val_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mx250torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
